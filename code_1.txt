#insert count>1 if header is present
import numpy as np
from itertools import combinations,chain

def powerset(s):
        return(list(chain.from_iterable(combinations(s,r) for r in range(1, len(s)+1))))
def write_rules(X,X_S,S,conf,sup,lift,num_trans):
        out_rules=""
        out_rules+="Freq. Itemset: {} \n".format(X)
        out_rules+= " Rule {} -> {} \n".format(list(S),list(X_S))
        out_rules+= " Conf: {0:2.3f}".format(conf)
        out_rules+= " Support: {0:2.3f}".format(sup/num_trans)
        out_rules+= " Lift: {0:2.3f}".format(lift)
        return out_rules

def load_transactions(path_to_data):
        trans=[]
        count=0
        with open(path_to_data,'r') as fi:
                for lines in fi:
                        count+=1
                        items=lines.strip().split(',')
                        pure_items=remove_null(items)
                        tt=list(np.unique(pure_items))
                        #tt.sort(key=lambda x: order.index(x))
                        if(count>1):
                                trans.append(tt)
        return trans

def remove_null(items):
        solid_item=[x for x in items if x != '' and x != '\n']
        return solid_item

def get_frequent(itemsets,Transactions,min_support,prev_discarded):
        l=[]
        supp_count=[]
        new_discarded= []
        num_trans=len(Transactions)
        k=len(prev_discarded.keys())

        for s in range(len(itemsets)):
                discarded_before = False
                if k > 0:
                        for it in prev_discarded[k]:
                                if(set(it).issubset(set(itemsets[s]))):
                                        discarded_before=True
                                        break
                if not discarded_before:
                        count=count_occurence(itemsets[s],Transactions)
                        if count/num_trans >= min_support:
                                l.append(itemsets[s])
                                supp_count.append(count)
                        else:
                                new_discarded.append(itemsets[s])
        return l,supp_count,new_discarded

def join_two_itemsets(it1,it2,order):
        it1.sort(key=lambda x: order.index(x))
        it2.sort(key=lambda x: order.index(x))
        for i in range(len(it1)-1):
                if it1[i] != it2[i]:
                        return []
        if order.index(it1[-1]) < order.index(it2[-1]):
                return it1 +[it2[-1]]
        return []

def join_set_itemsets(set_of_its,order):
        C=[]
        for i in range(len(set_of_its)):
                for j in range(i+1, len(set_of_its)):
                        it_out = join_two_itemsets(set_of_its[i],set_of_its[j],order)
                        if len(it_out) > 0:
                                C.append(it_out)
        return C
def count_occurence(itemset,transaction):
        count=0
        for i in range(len(transaction)):
                if set(itemset).issubset(set(transaction[i])):
                        count+=1
        return count
def print_table(T,supp_count):
                print("Itemset | Frequency")
                for k in range(len(T)):
                        print("{} : {}".format(T[k], supp_count[k]))
                print("\n\n")
if __name__ =='__main__':
        path_to_data='basket.csv'
        min_support=0.01
        min_conf=0.005
        cc=0
        with open(path_to_data,'r') as fi:
                all_items=[]
                for lines in fi:
                        cc+=1
                        for word in lines.strip().split(','):
                                if(word != '' and word != '\n' and cc>1):
                                        all_items.append(word.strip().lower())
                unique_list=list(set(all_items))
        transactions=load_transactions(path_to_data)
        C={}
        L={}
        itemset_size=1
        discarded={itemset_size : []}
        C.update({itemset_size :[ [f] for f in unique_list]})
        #print(C)
        supp_count_L={}
        f,sup,new_discarded=get_frequent(C[itemset_size],transactions,min_support,discarded)
        discarded.update({itemset_size :new_discarded})
        L.update({itemset_size : f})
        supp_count_L.update({itemset_size: sup})
        print("Table C1 : \n")
        print_table(L[1],supp_count_L[1])
        k=itemset_size+1
        convergence =False

        while not convergence:
                C.update({k : join_set_itemsets(L[k-1],unique_list)})
                print("Table C{} : \n".format(k))
                print_table(C[k], [count_occurence(it,transactions) for it in C[k]])
                f,sup,new_discarded =get_frequent(C[k],transactions,min_support,discarded)
                L.update({k : f})
                supp_count_L.update({k : sup})
                if len(L[k]) == 0:
                        convergence =True
                else:

                        print("Table L{} : \n".format(k))
                        print_table(L[k],supp_count_L[k])
                k+=1
        #generating the association rules
        assoc_rules_str=""
        for i in range(1,len(L)):
                for j in range(len(L[i])):
                        s=list(powerset(set(L[i][j])))
                        s.pop()
                for z in s:
                        S=set(z)
                        X=set(L[i][j])
                        X_S=set(X-S)
                        sup_x=count_occurence(X,transactions)
                        sup_x_s=count_occurence(X_S,transactions)
                        conf =sup_x /count_occurence(X_S, transactions)
                        conf=sup_x/count_occurence(S,transactions)
                        lift =conf/(sup_x_s/len(transactions))
                        if conf >= min_conf and sup_x >= min_support:
                                assoc_rules_str += write_rules(X,X_S,S,conf,sup_x,lift,len(transactions))
        print(assoc_rules_str)