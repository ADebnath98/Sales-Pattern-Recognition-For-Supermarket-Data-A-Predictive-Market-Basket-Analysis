import pandas as pd
import numpy as np
from mlxtend.frequent_patterns import apriori, association_rules
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier,plot_tree 
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
#import seaborn as sns
data = pd.read_csv('basket.csv')
data=data.head(500)
data=data.fillna('None',inplace=False)
items=data['0'].unique()
itemset = set(items)
encoded_vals = []
for index, row in data.iterrows():
    rowset = set(row) 
    labels = {}
    uncommons = list(itemset - rowset)
    commons = list(itemset.intersection(rowset))
    for uc in uncommons:
        labels[uc] = 0
    for com in commons:
        labels[com] = 1
    encoded_vals.append(labels)
ohe_df = pd.DataFrame(encoded_vals)

----------------------------------------------------------------------
#scaling and pca analysis

pima = ohe_df
scaler = StandardScaler()
scaled_data = pd.DataFrame(scaler.fit_transform(pima), columns=pima.columns)
scaled_data

---------------------------------------------------------------------------
#clustering 

kmeans = KMeans(n_clusters=7, n_init=10)
kmeans.fit(scaled_data)
clusters = kmeans.predict(scaled_data)
pca = PCA(n_components=8)
reduced_data = pd.DataFrame(pca.fit_transform(scaled_data), columns=['PC1', 'PC2','PC3','PC4','PC5','PC6','PC7','PC8'])
#reduced_data=scaled_data
reduced_centers = pca.transform(kmeans.cluster_centers_)
reduced_data['cluster'] = clusters
reduced_data
plt.figure(figsize=(14, 10))
plt.scatter(reduced_data[reduced_data['cluster'] == 0].loc[:, 'PC1'], reduced_data[reduced_data['cluster'] == 0].loc[:, 'PC8'], color='red')
plt.scatter(reduced_data[reduced_data['cluster'] == 1].loc[:, 'PC1'], reduced_data[reduced_data['cluster'] == 1].loc[:, 'PC8'], color='blue')
plt.scatter(reduced_data[reduced_data['cluster'] == 2].loc[:, 'PC1'], reduced_data[reduced_data['cluster'] == 2].loc[:, 'PC8'], color='yellow')
plt.scatter(reduced_data[reduced_data['cluster'] == 3].loc[:, 'PC1'], reduced_data[reduced_data['cluster'] == 3].loc[:, 'PC8'], color='orange')
plt.scatter(reduced_data[reduced_data['cluster'] == 4].loc[:, 'PC1'], reduced_data[reduced_data['cluster'] == 4].loc[:, 'PC8'], color='cyan')
plt.scatter(reduced_data[reduced_data['cluster'] == 5].loc[:, 'PC1'], reduced_data[reduced_data['cluster'] == 5].loc[:, 'PC8'], color='magenta')
plt.scatter(reduced_data[reduced_data['cluster'] == 6].loc[:, 'PC1'], reduced_data[reduced_data['cluster'] == 6].loc[:, 'PC2'], color='brown')
plt.scatter(reduced_centers[:, 0], reduced_centers[:, 1], color='black', marker='x', s=300)

plt.xlabel("PC1")
plt.ylabel("PC8")

plt.show()

----------------------------------------------------
#Decision Tree

import sklearn
from sklearn import tree
X=reduced_data.loc[:,reduced_data.columns!='cluster']
y = reduced_data.cluster 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
metrics.confusion_matrix(y_test,y_pred)
text_representation = tree.export_text(clf)
print(text_representation)
#_ = tree.plot_tree(clf)
plt=tree.plot_tree(clf)
plt